{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "import config\n",
    "import final_metric\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "TEXT_COLUMN = 'comment_preprocessed'\n",
    "EMBEDDINGS_PATH = 'data/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_comment_preprocessed.csv')\n",
    "train = train.sample(frac=1, random_state=7).reset_index(drop=True)\n",
    "train = train.head(config.NUM_SAMPLES)\n",
    "\n",
    "train['comment_preprocessed'] = train['comment_preprocessed'].astype(str) \n",
    "train['target'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n",
    "\n",
    "# All comments must be padded to be the same length.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 - 8s - loss: 0.2350 - acc: 0.9419 - val_loss: 0.2078 - val_acc: 0.9445 - 8s/epoch - 24ms/step\n",
      "Epoch 2/10\n",
      "313/313 - 4s - loss: 0.2103 - acc: 0.9422 - val_loss: 0.1966 - val_acc: 0.9445 - 4s/epoch - 12ms/step\n",
      "Epoch 3/10\n",
      "313/313 - 4s - loss: 0.1949 - acc: 0.9422 - val_loss: 0.1810 - val_acc: 0.9444 - 4s/epoch - 14ms/step\n",
      "Epoch 4/10\n",
      "313/313 - 4s - loss: 0.1802 - acc: 0.9427 - val_loss: 0.1761 - val_acc: 0.9466 - 4s/epoch - 14ms/step\n",
      "Epoch 5/10\n",
      "313/313 - 4s - loss: 0.1708 - acc: 0.9436 - val_loss: 0.1666 - val_acc: 0.9462 - 4s/epoch - 14ms/step\n",
      "Epoch 6/10\n",
      "313/313 - 4s - loss: 0.1629 - acc: 0.9452 - val_loss: 0.1616 - val_acc: 0.9482 - 4s/epoch - 12ms/step\n",
      "Epoch 7/10\n",
      "313/313 - 4s - loss: 0.1554 - acc: 0.9470 - val_loss: 0.1578 - val_acc: 0.9490 - 4s/epoch - 13ms/step\n",
      "Epoch 8/10\n",
      "313/313 - 4s - loss: 0.1490 - acc: 0.9490 - val_loss: 0.1605 - val_acc: 0.9474 - 4s/epoch - 14ms/step\n",
      "Epoch 9/10\n",
      "313/313 - 3s - loss: 0.1441 - acc: 0.9502 - val_loss: 0.1528 - val_acc: 0.9504 - 3s/epoch - 11ms/step\n",
      "Epoch 10/10\n",
      "313/313 - 4s - loss: 0.1396 - acc: 0.9512 - val_loss: 0.1514 - val_acc: 0.9508 - 4s/epoch - 14ms/step\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[config.TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[config.TOXICITY_COLUMN])\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                    EMBEDDINGS_DIMENSION,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(train_df, validate_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 250)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 250, 100)          4333800   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 250, 128)          25728     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 50, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 50, 128)           49280     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 10, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10, 128)           65664     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 1, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,491,242\n",
      "Trainable params: 157,442\n",
      "Non-trainable params: 4,333,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'CNN'\n",
    "y_pred = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]\n",
    "validate_df[MODEL_NAME] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- Model Performance: CNN ----------\n",
      "\n",
      "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  bnsp_auc\n",
      "2      homosexual_gay_or_lesbian             52      0.653409  0.772581  0.765403\n",
      "5                         muslim            124      0.688150  0.778919  0.792759\n",
      "3                      christian            211      0.731935  0.889428  0.669406\n",
      "4                         jewish             39      0.735714  0.679964  0.915166\n",
      "6                          black             93      0.775776  0.693163  0.893339\n",
      "1                         female            282      0.798221  0.796801  0.845759\n",
      "7                          white            140      0.803355  0.683475  0.909801\n",
      "8  psychiatric_or_mental_illness             22      0.835294  0.868578  0.804440\n",
      "0                           male            209      0.861640  0.799534  0.887842\n",
      "Final Metric: 0.7945709845269946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7945709845269946"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metric.get_value(validate_df, y_pred, MODEL_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
